<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png"><link rel="icon" href="/img/fluid.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="Fan Haolin"><meta name="keywords" content=""><meta name="description" content="激活函数（Activation Function)神经网络中将输入信号的总和转化为输出信号的的函数。其作用在于如何来激活输入信号的总和。It converts the sum of the input signal to the output signal. Its role is to determine how to activate the sum of input signals.The"><meta property="og:type" content="article"><meta property="og:title" content="Deep Learning-Activation Function"><meta property="og:url" content="http://example.com/posts/a2d2/index.html"><meta property="og:site_name" content="PhD Study"><meta property="og:description" content="激活函数（Activation Function)神经网络中将输入信号的总和转化为输出信号的的函数。其作用在于如何来激活输入信号的总和。It converts the sum of the input signal to the output signal. Its role is to determine how to activate the sum of input signals.The"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://example.com/.com//Sigmoid.png"><meta property="og:image" content="http://example.com/.com//Tanh.png"><meta property="article:published_time" content="2021-01-30T13:57:51.000Z"><meta property="article:modified_time" content="2023-01-27T13:25:31.662Z"><meta property="article:author" content="Fan Haolin"><meta property="article:tag" content="Python"><meta property="article:tag" content="Deep Learning"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="http://example.com/.com//Sigmoid.png"><title>Deep Learning-Activation Function - PhD Study</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var dntVal,CONFIG={hostname:"example.com",root:"/",version:"1.9.4",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1}},search_path:"/local-search.xml"};CONFIG.web_analytics.follow_dnt&&(dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on")))</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.4.2"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>XiangLu Action</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> <span>归档</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/default.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="Deep Learning-Activation Function"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2021-01-30 21:57" pubdate>2021年1月30日 晚上</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 3.5k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 29 分钟</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 style="display:none">Deep Learning-Activation Function</h1><div class="markdown-body"><blockquote><p><strong>激活函数</strong>（Activation Function)神经网络中将输入信号的总和转化为输出信号的的函数。其作用在于如何来激活输入信号的总和。<br>It converts the sum of the input signal to the output signal. Its role is to determine how to activate the sum of input signals.The activation function of neural network must use nonlinear function.</p></blockquote><span id="more"></span><iframe width="700" height="400" src="https://www.youtube.com/embed/WmbHsvzMaZc" title="宋祖英也跑了；為什麼抗蕭條救農村優先於城市？馬雲赴香港買了啥稀奇的「年貨」（文昭談古論今20230123第1198期）" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe><hr><h2 id="几种常见的激活函数以及Python实现"><a href="#几种常见的激活函数以及Python实现" class="headerlink" title="几种常见的激活函数以及Python实现"></a>几种常见的激活函数以及Python实现</h2><p>以下主要简要介绍几种常见的激活函数以及python实现</p><h3 id="Sigmoid-Function"><a href="#Sigmoid-Function" class="headerlink" title="Sigmoid Function"></a>Sigmoid Function</h3><p>Sigmoid 函数的平滑性对神经网络的学习具有重要意义<br>神经网络中常用的一个激活函数为Sigmoid 函数，其数学表达式为：<br>$$	Sigmoid(x) &#x3D; \dfrac{1}{1+e^{-x}} $$<br>其Python实现方法以及图示绘制为：</p><figure class="highlight python"><figcaption><span>3.6</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre class="line-numbers language-hljs python"><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> mpl_toolkits.axisartist <span class="hljs-keyword">as</span> axisartist<br><span class="hljs-keyword">import</span> matplotlib<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))<br>	<br>	<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">plot_sigmoid</span>():<br>    x = np.arange(-<span class="hljs-number">5.0</span>, <span class="hljs-number">5.0</span>, <span class="hljs-number">0.1</span>)<br>    y = sigmoid(x)<br>    plt.plot(x, y)<br>    plt.ylim(-<span class="hljs-number">0.1</span>, <span class="hljs-number">1.1</span>)<br>    plt.title(<span class="hljs-string">&#x27;Sigmoid Function&#x27;</span>)<br>    <span class="hljs-comment"># plt.xlabel(&#x27;X Axis&#x27;)</span><br>    <span class="hljs-comment"># plt.ylabel(&#x27;Y Axis&#x27;)</span><br>    <span class="hljs-comment"># plt.tight_layout()</span><br>    ax = plt.gca()<br>    ax.spines[<span class="hljs-string">&#x27;right&#x27;</span>].set_color(<span class="hljs-string">&#x27;none&#x27;</span>)<br>    ax.spines[<span class="hljs-string">&#x27;top&#x27;</span>].set_color(<span class="hljs-string">&#x27;none&#x27;</span>)<br>    ax.spines[<span class="hljs-string">&#x27;bottom&#x27;</span>].set_position((<span class="hljs-string">&#x27;data&#x27;</span>, <span class="hljs-number">0</span>))<br>    ax.spines[<span class="hljs-string">&#x27;left&#x27;</span>].set_position((<span class="hljs-string">&#x27;data&#x27;</span>, <span class="hljs-number">0</span>))<br>    ax.xaxis.set_ticks_position(<span class="hljs-string">&#x27;bottom&#x27;</span>)<br>    ax.yaxis.set_major_locator(matplotlib.ticker.MultipleLocator(<span class="hljs-number">0.5</span>))<br>    plt.savefig(<span class="hljs-string">&quot;sigmoid.png&quot;</span>)<br>    plt.show()<br>	<br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string"><code class="language-hljs python"><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> mpl_toolkits.axisartist <span class="hljs-keyword">as</span> axisartist<br><span class="hljs-keyword">import</span> matplotlib<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))<br>	<br>	<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">plot_sigmoid</span>():<br>    x = np.arange(-<span class="hljs-number">5.0</span>, <span class="hljs-number">5.0</span>, <span class="hljs-number">0.1</span>)<br>    y = sigmoid(x)<br>    plt.plot(x, y)<br>    plt.ylim(-<span class="hljs-number">0.1</span>, <span class="hljs-number">1.1</span>)<br>    plt.title(<span class="hljs-string">&#x27;Sigmoid Function&#x27;</span>)<br>    <span class="hljs-comment"># plt.xlabel(&#x27;X Axis&#x27;)</span><br>    <span class="hljs-comment"># plt.ylabel(&#x27;Y Axis&#x27;)</span><br>    <span class="hljs-comment"># plt.tight_layout()</span><br>    ax = plt.gca()<br>    ax.spines[<span class="hljs-string">&#x27;right&#x27;</span>].set_color(<span class="hljs-string">&#x27;none&#x27;</span>)<br>    ax.spines[<span class="hljs-string">&#x27;top&#x27;</span>].set_color(<span class="hljs-string">&#x27;none&#x27;</span>)<br>    ax.spines[<span class="hljs-string">&#x27;bottom&#x27;</span>].set_position((<span class="hljs-string">&#x27;data&#x27;</span>, <span class="hljs-number">0</span>))<br>    ax.spines[<span class="hljs-string">&#x27;left&#x27;</span>].set_position((<span class="hljs-string">&#x27;data&#x27;</span>, <span class="hljs-number">0</span>))<br>    ax.xaxis.set_ticks_position(<span class="hljs-string">&#x27;bottom&#x27;</span>)<br>    ax.yaxis.set_major_locator(matplotlib.ticker.MultipleLocator(<span class="hljs-number">0.5</span>))<br>    plt.savefig(<span class="hljs-string">&quot;sigmoid.png&quot;</span>)<br>    plt.show()<br>	<br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:<br>    plot_sigmoid()<br><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></span></pre></td></tr></table></figure><p>其结果展示如下：<br><img src="/.com//Sigmoid.png" srcset="/img/loading.gif" lazyload alt="Outcome for Sigmoid Function"></p><hr><h3 id="ReLU-Function"><a href="#ReLU-Function" class="headerlink" title="ReLU Function"></a>ReLU Function</h3><p><strong>ReLU 函数</strong>的全程为 Rectified Linear Unit，即意为<em>校正线性单元</em>。</p><p>其函数特征为：ReLU 函数在输入大于0时，直接输出该值；在输入小于0时，输出0，其python实现可以通过以下两种方式：</p><figure class="highlight python"><figcaption><span>3.6</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre class="line-numbers language-hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">relu</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> np.where(x &lt; <span class="hljs-number">0</span>, <span class="hljs-number"><code class="language-hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">relu</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> np.where(x &lt; <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, x)<br><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></span></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre class="line-numbers language-hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">relu</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> np.maximum(<span class="hljs-number"><code class="language-hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">relu</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> np.maximum(<span class="hljs-number">0</span>, x)<br><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></span></pre></td></tr></table></figure><blockquote><p>补充：<strong>shape函数</strong>的功能是读取矩阵的长度，比如shape[0]就是读取矩阵第一维度的长度,相当于行数。</p></blockquote><hr><h3 id="Tanh-Function"><a href="#Tanh-Function" class="headerlink" title="Tanh Function"></a>Tanh Function</h3><p>Tanh Function 的数学表达式为：<br>$$<br>\begin{align}<br>Tanh(x) &amp;&#x3D; \dfrac{e^{x}-e^{-x}}{e^{x}+e^{-x}}<br>\end{align}<br>$$</p><h2 id="其Python-的实现具体实现为：此函数的绘制思路与Sigmoid-函数基本相似，但此处本文采用区别于前的方法进行绘制：此种方法绘制的图线没有坐标轴因此可能缺乏严谨性，可以加以区分，其绘制的结果如下图所示："><a href="#其Python-的实现具体实现为：此函数的绘制思路与Sigmoid-函数基本相似，但此处本文采用区别于前的方法进行绘制：此种方法绘制的图线没有坐标轴因此可能缺乏严谨性，可以加以区分，其绘制的结果如下图所示：" class="headerlink" title="其Python 的实现具体实现为：此函数的绘制思路与Sigmoid 函数基本相似，但此处本文采用区别于前的方法进行绘制：此种方法绘制的图线没有坐标轴因此可能缺乏严谨性，可以加以区分，其绘制的结果如下图所示："></a>其Python 的实现具体实现为：<br><figure class="highlight python"><figcaption><span>3.6</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre class="line-numbers language-hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">tanh</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))<br></pre></td></tr></table></figure><br>此函数的绘制思路与Sigmoid 函数基本相似，但此处本文采用区别于前的方法进行绘制：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">x = np.arange(-<span class="hljs-number">6.0</span>, <span class="hljs-number">6.0</span>, <span class="hljs-number">0.1</span>)<br>y = tanh(x)<br>plt.plot(x, y)<br>plt.ylim(-<span class="hljs-number">1.5</span>, <span class="hljs-number">1.5</span>)<br>plt.title(<span class="hljs-string">&#x27;Tanh Function&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;X Axis&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Y Axis&#x27;</span>)<br><span class="hljs-comment"><code class="language-hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">tanh</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))<br></code></span></code></pre></td></tr></table></figure><br>此函数的绘制思路与Sigmoid 函数基本相似，但此处本文采用区别于前的方法进行绘制：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">x = np.arange(-<span class="hljs-number">6.0</span>, <span class="hljs-number">6.0</span>, <span class="hljs-number">0.1</span>)<br>y = tanh(x)<br>plt.plot(x, y)<br>plt.ylim(-<span class="hljs-number">1.5</span>, <span class="hljs-number">1.5</span>)<br>plt.title(<span class="hljs-string">&#x27;Tanh Function&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;X Axis&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Y Axis&#x27;</span>)<br><span class="hljs-comment">#plt.tight_layout()</span><br>plt.show()<br><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></td></tr></table></figure><br>此种方法绘制的图线<em>没有坐标轴</em>因此可能缺乏严谨性，可以加以区分，其绘制的结果如下图所示：<br><img src="/.com//Tanh.png" srcset="/img/loading.gif" lazyload alt="Outcome for Tanh Function"></h2><h2 id="线性函数与非线性函数"><a href="#线性函数与非线性函数" class="headerlink" title="线性函数与非线性函数"></a>线性函数与非线性函数</h2><p><strong>这里需要提到，Sigmoid 函数为非线性函数,阶跃函数也为非线性函数</strong></p><blockquote><p><em>尤其需要注意的是：激活函数不能使用线性函数，如果使用线性函数，加深神经网络的层数就不具有意义。</em></p></blockquote><p>　　使用线性函数的问题在于，不管如何加深层数，总是存在与之结构等效的“无隐藏层的神经网络。比如下面这个简单的例子（来自《鱼书》）：<br>　　我们将线性函数$h(x) &#x3D; cx$作为激活函数，将$y(x) &#x3D; h(h(h(x)))$的运算简单对应为3层神经网络（实际的神经网络更加复杂）。那么这个运算会进行$y(x) &#x3D; c\times c\times c\times x $的乘法运算，但是同样的运算可以由$y &#x3D; ax$[where: $a&#x3D;c^{3}$]这样的一次乘法运算表示。即表明，此次乘法运算可以由没有隐藏层到的神经网络表示，因而<strong>无法发挥多层网络</strong>的优势。<br>　　因此，为发挥叠加层所带来的优势，<strong>激活函数必须使用非线性函数</strong>。</p></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/Deep-Learning/" class="category-chain-item">Deep Learning</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/Python/">#Python</a> <a href="/tags/Deep-Learning/">#Deep Learning</a></div></div><div class="license-box my-3"><div class="license-title"><div>Deep Learning-Activation Function</div><div>http://example.com/posts/a2d2/</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>Fan Haolin</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2021年1月30日</div></div><div class="license-meta-item"><div>许可协议</div><div><a target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/posts/2adb/" title="FYP Application Introduction"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">FYP Application Introduction</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/posts/40f8/" title="Python PDF Generator"><span class="hidden-mobile">Python PDF Generator</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="" target="_blank" rel="nofollow noopener"><span>2023</span></a> <i class="iconfont icon-love"></i> <a href="" target="_blank" rel="nofollow noopener"><span>赵祥禄</span></a></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t){var e=Fluid.plugins.typing;(t=t.getElementById("subtitle"))&&e&&e(t.getAttribute("data-typed-text"))}((window,document))</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js",function(){var t,o=jQuery("#toc");0!==o.length&&window.tocbot&&(t=jQuery("#board-ctn").offset().top,window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-t},CONFIG.toc)),0<o.find(".toc-list-item").length&&o.css("visibility","visible"),Fluid.events.registerRefreshCallback(function(){var t;"tocbot"in window&&(tocbot.refresh(),0!==(t=jQuery("#toc")).length&&tocbot&&0<t.find(".toc-list-item").length&&t.css("visibility","visible"))}))})</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js",function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n,o=[];for(n of(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","))o.push(".markdown-body > "+n.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback(function(){if("anchors"in window){anchors.removeAll();var n,o=[];for(n of(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","))o.push(".markdown-body > "+n.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}})})</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",function(){Fluid.plugins.fancyBox()})</script><script>Fluid.plugins.imageCaption()</script><script src="/js/local-search.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>